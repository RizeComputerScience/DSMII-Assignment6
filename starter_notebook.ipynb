{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Building Your First Neural Network\n",
    "\n",
    "**Student Name:** [Your Name Here]\n",
    "\n",
    "**Date:** [Date]\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "You'll build feedforward neural networks to classify human activities from sensor data. You'll experiment with different architectures, compare performance against Unit 7's PCA+Random Forest approach, and learn when neural networks justify their added complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.7.2)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (8.9 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-macosx_12_0_arm64.whl (200.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.7/200.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp313-cp313-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp313-cp313-macosx_10_13_universal2.whl (676 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-macosx_11_0_arm64.whl (61 kB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-macosx_11_0_arm64.whl (346 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 wheel-0.45.1 wrapt-2.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Human Activity Recognition dataset\n",
    "# TODO: Load train.csv and test.csv from the data folder\n",
    "train_df = None  # Replace with pd.read_csv()\n",
    "test_df = None   # Replace with pd.read_csv()\n",
    "\n",
    "# TODO: Separate features from labels\n",
    "# The 'Activity' column contains the labels (1-6)\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "# Encode string labels to integers (0-5)\n",
    "# Neural networks need numerical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT: Verify dataset loaded correctly\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of activities: {len(np.unique(y_train))}\")\n",
    "print(f\"Activity labels range: {y_train.min()} to {y_train.max()}\")\n",
    "print(f\"Activity classes: {label_encoder.classes_}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features\n",
    "\n",
    "Neural networks learn best when features are normalized to similar scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use StandardScaler to normalize features\n",
    "# Fit on training data, transform both train and test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = None  # Replace with scaled training data\n",
    "X_test_scaled = None   # Replace with scaled test data\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT: Features Standardized\")\n",
    "print(f\"Mean of scaled features: {X_train_scaled.mean():.6f if X_train_scaled is not None else 'Not scaled'}\")\n",
    "print(f\"Std of scaled features: {X_train_scaled.std():.6f if X_train_scaled is not None else 'Not scaled'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Build and Train Baseline Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Baseline Architecture (1 Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a Sequential model with:\n",
    "# - Input layer: Dense(64, activation='relu', input_shape=(561,))\n",
    "# - Output layer: Dense(6, activation='softmax')\n",
    "\n",
    "baseline_model = None  # Replace with your model\n",
    "\n",
    "# TODO: Compile the model with:\n",
    "# - optimizer='adam'\n",
    "# - loss='sparse_categorical_crossentropy'\n",
    "# - metrics=['accuracy']\n",
    "\n",
    "\n",
    "# Display model architecture\n",
    "if baseline_model is not None:\n",
    "    baseline_model.summary()\n",
    "else:\n",
    "    print(\"Model not created yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model for 20 epochs with validation_split=0.2\n",
    "# Save the training history and training time\n",
    "\n",
    "print(\"Training baseline model (1 hidden layer, 64 nodes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Your training code here\n",
    "baseline_history = None  # Replace with model.fit() result\n",
    "\n",
    "baseline_time = time.time() - start_time\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "baseline_test_loss, baseline_test_accuracy = None, None  # Replace with model.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Architecture: 1 hidden layer, 64 nodes\")\n",
    "print(f\"Test accuracy: {baseline_test_accuracy if baseline_test_accuracy else 'Not calculated'}\")\n",
    "print(f\"Training time: {baseline_time:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Experiment with Network Depth (Number of Layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Network (1 Layer) - Already Done Above\n",
    "\n",
    "Baseline model = 1 hidden layer with 64 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Network (2 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build model with 2 hidden layers:\n",
    "# - Dense(128, activation='relu', input_shape=(561,))\n",
    "# - Dense(64, activation='relu')\n",
    "# - Dense(6, activation='softmax')\n",
    "\n",
    "medium_model = None  # Replace with your model\n",
    "\n",
    "# TODO: Compile with same settings as baseline\n",
    "\n",
    "\n",
    "# TODO: Train for 20 epochs with validation_split=0.2\n",
    "print(\"Training medium network (2 hidden layers)...\")\n",
    "medium_history = None  # Replace with model.fit() result\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "medium_test_loss, medium_test_accuracy = None, None\n",
    "\n",
    "print(f\"\\nMedium network test accuracy: {medium_test_accuracy if medium_test_accuracy else 'Not calculated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Network (3 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build model with 3 hidden layers:\n",
    "# - Dense(128, activation='relu', input_shape=(561,))\n",
    "# - Dense(64, activation='relu')\n",
    "# - Dense(32, activation='relu')\n",
    "# - Dense(6, activation='softmax')\n",
    "\n",
    "deep_model = None  # Replace with your model\n",
    "\n",
    "# TODO: Compile with same settings\n",
    "\n",
    "\n",
    "# TODO: Train for 20 epochs with validation_split=0.2\n",
    "print(\"Training deep network (3 hidden layers)...\")\n",
    "deep_history = None  # Replace with model.fit() result\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "deep_test_loss, deep_test_accuracy = None, None\n",
    "\n",
    "print(f\"\\nDeep network test accuracy: {deep_test_accuracy if deep_test_accuracy else 'Not calculated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Depth Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison of depth experiments\n",
    "depth_results = pd.DataFrame({\n",
    "    'Architecture': ['1 Layer (64)', '2 Layers (128, 64)', '3 Layers (128, 64, 32)'],\n",
    "    'Test Accuracy': [baseline_test_accuracy, medium_test_accuracy, deep_test_accuracy]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEPTH EXPERIMENT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(depth_results.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Network Depth\n",
    "\n",
    "**How does adding more layers affect performance? At what point do you see diminishing returns? (2-3 sentences)**\n",
    "\n",
    "[Write your response here. Consider: Did accuracy improve with each added layer? Was the improvement significant? Is there a point where adding layers doesn't help much?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Experiment with Network Width (Nodes Per Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Network (2 Layers, 32 Nodes Each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build model with 2 layers of 32 nodes each\n",
    "# - Dense(32, activation='relu', input_shape=(561,))\n",
    "# - Dense(32, activation='relu')\n",
    "# - Dense(6, activation='softmax')\n",
    "\n",
    "narrow_model = None  # Replace with your model\n",
    "\n",
    "# TODO: Compile and train for 20 epochs\n",
    "print(\"Training narrow network (2 layers, 32 nodes each)...\")\n",
    "narrow_history = None\n",
    "\n",
    "# TODO: Evaluate\n",
    "narrow_test_loss, narrow_test_accuracy = None, None\n",
    "\n",
    "print(f\"\\nNarrow network test accuracy: {narrow_test_accuracy if narrow_test_accuracy else 'Not calculated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Width Network (2 Layers, 64 Nodes Each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build model with 2 layers of 64 nodes each\n",
    "medium_width_model = None\n",
    "\n",
    "# TODO: Compile and train\n",
    "print(\"Training medium-width network (2 layers, 64 nodes each)...\")\n",
    "medium_width_history = None\n",
    "\n",
    "# TODO: Evaluate\n",
    "medium_width_test_loss, medium_width_test_accuracy = None, None\n",
    "\n",
    "print(f\"\\nMedium-width network test accuracy: {medium_width_test_accuracy if medium_width_test_accuracy else 'Not calculated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Network (2 Layers, 128 Nodes Each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build model with 2 layers of 128 nodes each\n",
    "wide_model = None\n",
    "\n",
    "# TODO: Compile and train\n",
    "print(\"Training wide network (2 layers, 128 nodes each)...\")\n",
    "wide_history = None\n",
    "\n",
    "# TODO: Evaluate\n",
    "wide_test_loss, wide_test_accuracy = None, None\n",
    "\n",
    "print(f\"\\nWide network test accuracy: {wide_test_accuracy if wide_test_accuracy else 'Not calculated'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Width Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison of width experiments\n",
    "width_results = pd.DataFrame({\n",
    "    'Architecture': ['2 Layers (32 each)', '2 Layers (64 each)', '2 Layers (128 each)'],\n",
    "    'Test Accuracy': [narrow_test_accuracy, medium_width_test_accuracy, wide_test_accuracy]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WIDTH EXPERIMENT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(width_results.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Network Width\n",
    "\n",
    "**How does increasing nodes per layer affect performance? Is there a point where adding more nodes doesn't help? (2-3 sentences)**\n",
    "\n",
    "[Write your response here. Consider: Did wider layers improve accuracy? Was the improvement worth the added complexity? Where do you see diminishing returns?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Train Best Architecture and Visualize Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Train Best Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Based on your experiments above, build your best-performing architecture\n",
    "# Train it for 30 epochs (longer training often improves performance)\n",
    "\n",
    "best_model = None  # Replace with your best architecture\n",
    "\n",
    "# TODO: Compile the model\n",
    "\n",
    "\n",
    "print(\"Training best model for 30 epochs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# TODO: Train for 30 epochs with validation_split=0.2\n",
    "best_history = None  # Replace with model.fit() result\n",
    "\n",
    "best_time = time.time() - start_time\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "best_test_loss, best_test_accuracy = None, None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Architecture: [Describe your architecture here]\")\n",
    "print(f\"Test accuracy: {best_test_accuracy if best_test_accuracy else 'Not calculated'}\")\n",
    "print(f\"Training time: {best_time:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training accuracy vs validation accuracy over epochs\n",
    "# Use best_history.history['accuracy'] and best_history.history['val_accuracy']\n",
    "\n",
    "if best_history is not None:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Your plotting code here\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train best model first to visualize results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training loss vs validation loss over epochs\n",
    "# Use best_history.history['loss'] and best_history.history['val_loss']\n",
    "\n",
    "if best_history is not None:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Your plotting code here\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train best model first to visualize results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Compare Against Unit 7 PCA + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your Unit 7 PCA+Random Forest results here\n",
    "# You should have recorded these in Unit 7\n",
    "unit7_pca_rf_accuracy = None  # Replace with your Unit 7 test accuracy\n",
    "unit7_pca_rf_time = None  # Replace with your Unit 7 training time (if you have it)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Baseline NN (1 layer, 64 nodes)',\n",
    "        'Best NN (Your Architecture)',\n",
    "        'Unit 7: PCA + Random Forest'\n",
    "    ],\n",
    "    'Features/Layers': [\n",
    "        '1 hidden layer',\n",
    "        '[Describe your architecture]',\n",
    "        '30 PCA components'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        baseline_test_accuracy,\n",
    "        best_test_accuracy,\n",
    "        unit7_pca_rf_accuracy\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        f\"{baseline_time:.2f}s\" if baseline_time else 'N/A',\n",
    "        f\"{best_time:.2f}s\" if best_time else 'N/A',\n",
    "        f\"{unit7_pca_rf_time:.2f}s\" if unit7_pca_rf_time else 'N/A'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: NEURAL NETWORKS VS UNIT 7\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection on Neural Network vs PCA+Random Forest\n",
    "\n",
    "**How does your best neural network compare to PCA+Random Forest? Did the neural network's ability to learn its own features from raw sensor data lead to better performance than manually engineering features with PCA? (2-3 sentences)**\n",
    "\n",
    "[Write your response here. Consider: Which approach achieved higher accuracy? Was the difference significant? What are the tradeoffs in complexity, training time, and interpretability?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Reflect on When to Use Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Neural Networks vs Simpler Models\n",
    "\n",
    "**Based on your results and what you learned in the resources, when should you use neural networks versus simpler models like random forests? Consider factors like data type, dataset size, interpretability needs, and performance requirements. Give at least one example of when you'd choose neural networks and one example of when you'd choose random forests. (3-4 sentences)**\n",
    "\n",
    "[Write your response here. Think about:\n",
    "- What types of data are neural networks best for? (images, sensor data, text vs tabular business data)\n",
    "- When do neural networks justify their added complexity and training time?\n",
    "- When would simpler models like random forests be more appropriate?\n",
    "- Give concrete examples of problems where you'd choose each approach]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Submit Your Work\n",
    "\n",
    "Before submitting:\n",
    "1. Make sure all code cells run without errors\n",
    "2. Verify you have:\n",
    "   - Baseline neural network trained and evaluated\n",
    "   - At least 3 depth experiments (1, 2, 3 layers)\n",
    "   - At least 3 width experiments (32, 64, 128 nodes)\n",
    "   - Best model trained for 30 epochs with accuracy and loss plots\n",
    "   - Comparison table with baseline NN, best NN, and Unit 7 PCA+RF\n",
    "   - All reflection questions answered (2-3 or 3-4 sentences each)\n",
    "3. Check that all visualizations display correctly\n",
    "\n",
    "Then push to GitHub:\n",
    "```bash\n",
    "git add .\n",
    "git commit -m 'completed neural networks assignment'\n",
    "git push\n",
    "```\n",
    "\n",
    "Submit your GitHub repository link on the course platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
